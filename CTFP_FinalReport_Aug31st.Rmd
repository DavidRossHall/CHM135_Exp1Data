---
title: "CTFP Final Report"
subtitle: "Excel-erating Data Analysis Skills in an Introductory Chemistry Lab"
author: "David Hall and Dr. J. D'eon (supervisor)"
date: "`r Sys.Date()`"
output:
  tufte::tufte_handout:
    extra_dependencies: ["float"]
    citation_package: natbib
    latex_engine: xelatex
  tufte::tufte_html: default
  tufte::tufte_book:
    citation_package: natbib
    latex_engine: xelatex
bibliography: CTFP.bib
link-citations: yes
header-includes:
  - \setcitestyle{numbers}
  - \usepackage{float}
  - \usepackage{amsmath}
---

```{r setup, include=FALSE}
library(tufte)
# invalidate cache when the tufte version changes
knitr::opts_chunk$set(tidy = FALSE, cache.extra = packageVersion('tufte'))
options(htmltools.dir.version = FALSE)

library(tidyverse)
library(ggpubr)
library(ggpmisc)
library(ggExtra)
library(openxlsx)
library(RcppRoll)
library(cowplot)
library(kableExtra)
library(gridExtra)
```

# Introduction

Whether you like it or not, we are living in an increasingly data centric world, and the field of chemistry is no exception. An oft overlooked aspect of this is how exactly data (measurements, signals, etc) is transformed into information (trends, correlation) and finally into knowledge. Moreover, the explicit teaching of these concepts is often neglected resulting in increasing student frustration.[@Schlotter2013] Motivated by this, and the need to transfer to a virtual laboratory environment as a result of Covid-19, we saught to develop a new, remote learning compatible, experiment for *CHM 135: Physical Principles*. 


*Experiment 1: The Chemistry of Air Quality* is the results of our efforts. In this new experiment first-year students are introduced to fundamental data analysis concepts as they explore some of the chemistry of airborn pollutants. 

# Chemistry background

Since 1975 Environment and Climate Change Canada (ECCC) has been monitoring several airborn pollutants through the National Airborne Pollutant Surveillance (NAPS) program. Two of the key pollutants monitored are ozone (O~3~) and nitrogen dioxide (NO~2~), and whose interdependent diurnal cycles are expressed through equations 1 to 3, right. The relationship between O~3~ and NO~2~ is so intimate, atmospheric chemist have developed the tern "odd oxygen", O~x~, as the sum of these two components (equation 4).[@Kley1994] Lastly, the correlation between O~3~ and NO~2~ varies with environmetal influences such as increased levels of volatile organic compounds and temperature engendered during the summer months. Through the NAPS data, students can visualize and qualitatively assess these relationships. 



```{marginfigure}
\begin{align}
  NO_2 + photon &\rightarrow NO + O \\
  O + O_2 &\rightarrow O_3 \\
  O_3 + NO &\rightarrow O_2 + NO_2 \\
  [O_3] + [NO_2] &= [O_x]
\end{align}
```

# Experiment workflow

Operationally, after an introductory pre-lab prepared by Dr. D'eon (with accompanying video and gas phase chemistry questions), each student analyzes one randomly assigned winter and summer data sets from a pool upload and distributed through Quercus. Each dataset comprises a 7-day snapshot of O~3~ and NO~2~ concentrations as measured by a downtown Toronto monitoring station from the NAPS program.^[A more technical explanation of how NAPS data was subseted for students can be found at page 4.] The experiment instructions, and a supporting *Tip Sheet* on operations in Excel necessary for Experiment 1, guide students through the data analysis workflow made popular by Wickham and Grolemund.[@Wickham2017] 

![Data exploration/analysis workflow; figure from *R for Data Science* (2017)](images/data-science-workflow.png)

- *Importing* their assigned comma separate values (.csv) data sets into Excel.
- *Tidying* their data and setting up their worksheets. This step consist of formatting cells to properly display values and handling missing data.^[Specifically, NAPS stores missing values as -999, but this value is literally interpreted by Excel, requiring removal before further data visualization/analysis.]
- *Visualizing* their quantitative information through a time-series plot of time vs. concentration of pollutant. 
- *Transforming* their data using mathematical operators in Excel to calculate total oxidant and adding it to their time-series plot as well as calculating 8 hr moving averages. 
- *Modelling* a linear relationship between O~3~ and NO~2~ to qualitatively assess the inversal relationship between these two contaminants.^[This is accomplished using the "add trend line" function in Excel, although previous versions of the lab utilized the "linear regression" function of the *Analysis Toolpak*.]
- *Communicating* and exploring their results through a series of accompanying questions written by Dr. J. D'eon. 

# Expected student outcomes 

Beyond the introduction to gas-phase and atmospheric chemistry, students are expected to learn the basics of data analysis and operations in Microsoft Excel. through the Experiment and supporting *Tip Sheet*. Topics covered include: cell referencing, mathematical operators, *find and replace* functions, cell formating, plotting, and summary statistics. While only touching the surface of data analysis, we believe this list touches upon the most frequently used operations in Excel, and provides a solid base from which students can improve their understanding and skills on their own or in future classes. 

Verification of student learning is assessed through through the results of each students data analysis. For each data set, students are expected to plot a time-series of polutant concentration and a correlation plot of O~3~ and NO~2~ with linear regression (Figure 2A and 2B). Additionnaly they perform the same analysis on both the winter and summer datasets, illustrating the increasing complexity of summer vs. winter atmospheric chemistry (Figure 2C). Through their visulizations, students must answer a series of accompanying questions wherein they inquire about possible explanations for the differences in their winter and summer results. Any faults in a student's data analysis are readily aparent in their visualations. For example, an errant '-999' value leftover from the NAPS dataset is easily visible to the TA (Figure 2D). Lastly, every student dataset that is generated is accompanied with by a PDF anwser sheet containing a time-series plot, correlation plots, and all summary statistics students are expedcted to perform. Consequently TAs can simply compare a students analysis of a given datasets against the accompanying anwser sheet to quickly verify their work. How the answer sheets are generated is discussed below.

```{r,fig.margin=TRUE, echo = FALSE, message = FALSE, warning=FALSE, fig.height = 7, fig.cap = "Example of plots students are expected to create. (A) time-series of pollutants across 7 winter days. (B) Correlation plot of O3 and NO2 concentrations with linear regression in the winter and (C) summer. (D) Example plot if a '-999' value wasn't removed."}
data <- read.csv("Toronto_60410_2018/Toronto_60410_2018_Day10to16.csv", header = TRUE)
dataSummer <- read.csv("Toronto_60410_2018/Toronto_60410_2018_Day189to195.csv", header = TRUE)

data <- data %>%
  mutate(time = convertToDateTime(data$Date, origin = "1900-01-01")) %>%
  filter(O3 != -999) %>%
  filter(NO2 != -999) %>%
  mutate(OX = NO2 + O3)

### Making data tidyR friendly --------------------------------------------------
dataCol <- data %>%
  select(-c("Date")) %>%
  pivot_longer(-time, names_to = "pollutant", values_to = "concentration")


dataSummer <- dataSummer %>%
  mutate(time = convertToDateTime(dataSummer$Date, origin = "1900-01-01")) %>%
  filter(O3 != -999) %>%
  filter(NO2 != -999) %>%
  mutate(OX = NO2 + O3)

### Making data tidyR friendly --------------------------------------------------
dataSummerCol <- dataSummer %>%
  select(-c("Date")) %>%
  pivot_longer(-time, names_to = "pollutant", values_to = "concentration")

### Time series ----------------
a <- ggplot(data = dataCol, aes(x = time, y = concentration, color = pollutant)) +
  geom_line(size = 1) +
  theme_classic() +
   theme(text = element_text(size = 12),
         legend.position = "right") +
  ylab(bquote('Conc., ppb')) +
  xlab(bquote('Time')) 

### Correlation plot with Linear regression and equation -------------------------

formula <- y ~ x ### Need to keep this so LM regression appears on plot

b <- ggplot(data = data, aes(x = NO2, y = O3)) +
  geom_point(size = 0.5) + 
  scale_x_continuous(expand = c(0, 0), limits = c(0, 45)) + 
  scale_y_continuous(expand = c(0, 0), limits = c(0, 70)) +
  theme_classic() +
  theme(text = element_text(size = 12))+
  xlab(bquote('Conc.' ~NO[2]~', ppb')) +
  ylab(bquote('Conc.' ~O[3]~', ppb')) +
  geom_smooth(method = "lm", formula = formula, se = FALSE) +
    stat_poly_eq(aes(label =  paste(stat(rr.label), sep = "*\", \"*")),
               formula = formula, rr.digits = 4 , parse = TRUE, label.y = 0.25, label.x = 0.95, size = 4)+
  annotate("text", x =35, y = 33, label = "winter")

c <- ggplot(data = dataSummer, aes(x = NO2, y = O3)) +
  geom_point(size = 0.5) + 
  scale_x_continuous(expand = c(0, 0), limits = c(0, 45)) + 
  scale_y_continuous(expand = c(0, 0), limits = c(0, 70)) +
  theme_classic() +
  theme(text = element_text(size = 12))+
  xlab(bquote('Conc.' ~NO[2]~', ppb')) +
  ylab(bquote('Conc.' ~O[3]~', ppb')) +
  geom_smooth(method = "lm", formula = formula, se = FALSE) +
    stat_poly_eq(aes(label =  paste(stat(rr.label), sep = "*\", \"*")),
               formula = formula, rr.digits = 4 , parse = TRUE, label.y = 0.25, label.x = 0.95, size = 4) +
  annotate("text", x =36, y = 33, label = "summer")

# -999 value for example

dataSummer[2,3] = -999
  
d <- ggplot(data = dataSummer, aes(x = NO2, y = O3)) +
  geom_point(size = 0.5) + 
  scale_x_continuous(expand = c(0, 0), limits = c(0, 45)) + 
  scale_y_continuous(expand = c(0, 0), limits = c(-1050, 70)) +
  theme_classic() +
  theme(text = element_text(size = 12))+
  xlab(bquote('Conc.' ~NO[2]~', ppb')) +
  ylab(bquote('Conc.' ~O[3]~', ppb')) +
  geom_segment(aes(x = 22, y = -600, xend = 16.5, yend = -950),
                  arrow = arrow(length = unit(0.5, "cm"))) +
  annotate("text", x = 32, y = -550, label = "error from analysis")

gt <- arrangeGrob(a, b, c, d,                               
             ncol = 1, nrow = 4)
# Add labels to the arranged plots
p <- as_ggplot(gt) +                                # transform to a ggplot
  draw_plot_label(label = c("A", "B", "C", "D"), size = 16,
                  x = c(0, 0, 0, 0), y = c(1, 0.78, 0.53, 0.28)) # Add labels
p

```

# Lab Results 

We were unable to introduce a survey to undergradaute students that would have explicitly addressed their experinences and thoughs with Experiment 1. However, discussing Experiment 1 with the four laboratory TAs revealed two promising insights. Firstly, the TAs found that students' questions were largely related to lab content, and not the technical aspects of Excel. Secondly, student questions that did pertain to Excel were readily addressed by directing them to the supporting *Tip sheet* document. 

Furthermore, after inspection of more than 300 student submitted figures, none had any critical flaws (either in visualization or data analysis). There were instances of several minor issues common to several student plots (i.e. adding a linear regression to their time-series plots), but these have been addressed in the updated Experiment 1 instructions and supporting *Tip Sheet* for the Fall 2020 session.

# Implementation in the Fall Term 

Experiment 1 is slated to be introduced in the upcoming Fall 2020 CHM 135 session, with an estimated >1500 enrolled students. Fortunately, the scope of the NAPS program and the readily scalable coding used to generate student datasets/answer sheets can easily cope with the increase in course participants. As well, a limitation of the summer session was that limitations in the Quercus setup practically limited the entire course to 15 winter and 15 summer datasets. This has been addressed in the upcoming session where each lab section will have it's own unique pool of datasets from a unique NAPS monitoring station in Toronto. While we'd ideally like to assign each student with a unique dataset, this method best minimizes overlap in student analysis while remaining practical in the existing Quercus infrastructure. 

- Dialing in Quercus setup to expand course components to > 1800 students. 
  - can create that many data sets/answer keys easily, bottleneck is upload to quercus. 
- refine Excel operations
- Explicit discussions on data analysis
  - this lab, in my opinion, is more about understanding data analysis than it is chemistry. Once you learn stuff here you can apply in all sorts of ways in upper year courses. 
  - That's why the *Tip Sheet* i wrote was so long. I never expected students to read the entire thing, but if they had any questions they could look it up there. To be fair though, I think the info within that document should prettied up and set up as a departmental wide guide to data visualization/analysis etc. Some of the stuff made by grad students is awful/deceptive. 
- Enhanced discussion on statistics with a focus on interpreting the numbers rather then calculating them with mathematical formulas. 

# Stuff left on the cutting room floor 

* tried a bunch of stuff, and left plenty on the cutting room floor
  + SO2 work 
  + using Analysis Toolpak for linear regression (outputs additional parameters hidden from display line of best fit)
  





# Source code and instructions for generating datasets

The source code and example ECCC data, student datasets, and TA answer reports can all be found on [GitHub](https://github.com/DavidRossHall/CHM135_Exp1Data).^[Github link: [https://github.com/DavidRossHall/CHM135_Exp1Data](https://github.com/DavidRossHall/CHM135_Exp1Data)]

**NEED to explain GitHub better ** I feel like this is goign to be lost on folks, when it actually solves so many issues about the way information is passed along between faculty and over the years. 

GitHub provides hosting for software developement, distribution version control, and source code management and is readily [integretated into the RStudio environment](https://support.rstudio.com/hc/en-us/articles/200532077-Version-Control-with-Git-and-SVN).  In practice, this means that the code used to automatically genereate student datasets and anwser keys is preserved online, and safely passed along from year to year. The GitHub environment is ideal for introducing new componenets and removing old ones from the code thanks to version control. This is expecially important as faculty frequently rotate through the CHM 135 course. 

# Brief discussion on how datasets are generated, what transformations need to be applied, etc. 

```{r, echo = FALSE}
dataCSV <- read.csv("Toronto_60410_2018/Toronto_60410_2018_Day10to16.csv", header = TRUE)

knitr::kable(head(dataCSV[, ]), 
             "simple",
             digits = c(4, 0, 0),
             caption = " A tibble of a student assigned dataset; note the Excel complient data & time formats.",
             booktabs = T)


```

```{r bib, include=FALSE}
# create a bib file for the R packages used in this document
knitr::write_bib(c('base', 'rmarkdown'), file = 'skeleton.bib')
```
