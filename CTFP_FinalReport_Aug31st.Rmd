---
title: "CTFP Final Report"
subtitle: "Excel-erating Data Analysis Skills in an Introductory Chemistry Lab"
author: "David Hall and Dr. J. D'eon (supervisor)"
date: "`r Sys.Date()`"
output:
  tufte::tufte_handout:
    extra_dependencies: ["float"]
    citation_package: natbib
    latex_engine: xelatex
  tufte::tufte_html: default
  tufte::tufte_book:
    citation_package: natbib
    latex_engine: xelatex
bibliography: CTFP.bib
link-citations: yes
header-includes:
  - \setcitestyle{numbers}
  - \usepackage{float}
  - \usepackage{amsmath}
---

```{r setup, include=FALSE}
library(tufte)
# invalidate cache when the tufte version changes
knitr::opts_chunk$set(tidy = FALSE, cache.extra = packageVersion('tufte'))
options(htmltools.dir.version = FALSE)

library(tidyverse)
library(ggpubr)
library(ggpmisc)
library(ggExtra)
library(openxlsx)
library(RcppRoll)
library(cowplot)
library(kableExtra)
library(gridExtra)
library(grid)
```

# Introduction

Whether we like it or not, we are living in an increasingly data centric world, and the field of chemistry is no exception. An oft overlooked aspect of this is how exactly data (measurements, signals, etc) is transformed into information (trends, correlation) and finally into knowledge. Moreover, the explicit teaching of these concepts is often neglected resulting in increasing student frustration.[@Schlotter2013] Motivated by this, and the need to transfer to a virtual laboratory environment as a result of Covid-19, we sought to develop a new, remote learning compatible, experiment for *CHM 135: Physical Principles*. 


*Experiment 1: The Chemistry of Air Quality* is the results of our efforts. In this new experiment first-year students are introduced to fundamental data analysis concepts as they explore some of the chemistry of airborne pollutants. 

# Chemistry background

Since 1975 Environment and Climate Change Canada (ECCC) has been monitoring several airborne pollutants through the National Airborne Pollutant Surveillance (NAPS) program. Two of the key pollutants monitored are ozone (O~3~) and nitrogen dioxide (NO~2~), and whose interdependent diurnal cycles are expressed through equations 1 to 3, right. The relationship between O~3~ and NO~2~ is so intimate, atmospheric chemist have developed the tern "odd oxygen", O~x~, as the sum of these two components (equation 4).[@Kley1994] Lastly, the correlation between O~3~ and NO~2~ varies with environmental influences such as increased levels of volatile organic compounds and temperature engendered during the summer months. Through the NAPS data, students can visualize and qualitatively assess these relationships. 



```{marginfigure}
\begin{align}
  NO_2 + photon &\rightarrow NO + O \\
  O + O_2 &\rightarrow O_3 \\
  O_3 + NO &\rightarrow O_2 + NO_2 \\
  [O_3] + [NO_2] &= [O_x]
\end{align}
```

# Experiment workflow

Operationally, after an introductory pre-lab prepared by Dr. D'eon (with accompanying video and gas phase chemistry questions), each student analyzes one randomly assigned winter and summer data sets from a pool upload and distributed through Quercus. Each data set comprises a 7-day snapshot of O~3~ and NO~2~ concentrations as measured by a downtown Toronto monitoring station from the NAPS program.^[A more technical explanation of how NAPS data was subseted for students can be found on page 6.] The experiment instructions, and a supporting *Tip Sheet* on operations in Excel necessary for Experiment 1, guide students through the data analysis workflow made popular by Wickham and Grolemund.[@Wickham2017] 

![Data exploration/analysis workflow; figure from *R for Data Science* (2017)](images/data-science-workflow.png)

- *Importing* their assigned comma separate values (.csv) data sets into Excel.
- *Tidying* their data and setting up their worksheets. This step consist of formatting cells to properly display values and handling missing data.^[Specifically, NAPS stores missing values as -999, but this value is literally interpreted by Excel, requiring removal before further data visualization/analysis.]
- *Visualizing* their quantitative information through a time-series plot of time vs. concentration of pollutant. 
- *Transforming* their data using mathematical operators in Excel to calculate total oxidant and adding it to their time-series plot as well as calculating 8 hr moving averages. 
- *Modelling* a linear relationship between O~3~ and NO~2~ to qualitatively assess the negative relationship between these two contaminants.^[This is accomplished using the "add trend line" function in Excel, although previous versions of the lab utilized the "linear regression" function of the *Analysis Toolpak*.]
- *Communicating* and exploring their results through a series of accompanying questions written by Dr. J. D'eon. 

# Expected student outcomes 

Beyond the introduction to gas-phase and atmospheric chemistry, students are expected to learn the basics of data analysis and operations in Microsoft Excel. through the Experiment and supporting *Tip Sheet*. Topics covered include: cell referencing, mathematical operators, *find and replace* functions, cell formatting, plotting, and summary statistics. While only touching the surface of data analysis, we believe this list touches upon the most frequently used operations in Excel, and provides a solid base from which students can improve their understanding and skills on their own or in future classes. 

Verification of student learning is assessed through through the results of each students data analysis. For each data set, students are expected to plot a time-series of pollutant concentration and a correlation plot of O~3~ and NO~2~ with linear regression (Figure 2A and 2B). Additionally they perform the same analysis on both the winter and summer data sets, illustrating the increasing complexity of summer vs. winter atmospheric chemistry (Figure 2C). Through their visualizations, students must answer a series of accompanying questions wherein they inquire about possible explanations for the differences in their winter and summer results. Any faults in a student's data analysis are readily apparent in their visualizations. For example, an errant '-999' value leftover from the NAPS data set is easily visible to the TA (Figure 2D). Lastly, every student data set that is generated is accompanied by a PDF answer sheet containing a time-series plot, correlation plots, and all summary statistics students are expected to perform. Consequently TAs can simply compare a students analysis of a given data sets against the accompanying answer sheet to quickly verify their work. How the answer sheets are generated is discussed below.

```{r,fig.margin=TRUE, echo = FALSE, message = FALSE, warning=FALSE, fig.height = 7, fig.cap = "Example of plots students are expected to create. (A) time-series of pollutants across 7 winter days. (B) Correlation plot of O3 and NO2 concentrations with linear regression in the winter and (C) summer. (D) Example plot if a '-999' value wasn't removed."}
data <- read.csv("Toronto_60410_2018/Toronto_60410_2018_Day10to16.csv", header = TRUE)
dataSummer <- read.csv("Toronto_60410_2018/Toronto_60410_2018_Day189to195.csv", header = TRUE)

data <- data %>%
  mutate(time = convertToDateTime(data$Date, origin = "1900-01-01")) %>%
  filter(O3 != -999) %>%
  filter(NO2 != -999) %>%
  mutate(OX = NO2 + O3)

### Making data tidyR friendly --------------------------------------------------
dataCol <- data %>%
  select(-c("Date")) %>%
  pivot_longer(-time, names_to = "pollutant", values_to = "concentration")


dataSummer <- dataSummer %>%
  mutate(time = convertToDateTime(dataSummer$Date, origin = "1900-01-01")) %>%
  filter(O3 != -999) %>%
  filter(NO2 != -999) %>%
  mutate(OX = NO2 + O3)

### Making data tidyR friendly --------------------------------------------------
dataSummerCol <- dataSummer %>%
  select(-c("Date")) %>%
  pivot_longer(-time, names_to = "pollutant", values_to = "concentration")

### Time series ----------------
a <- ggplot(data = dataCol, aes(x = time, y = concentration, color = pollutant)) +
  geom_line(size = 1) +
  theme_classic() +
   theme(text = element_text(size = 12),
         legend.position = "right") +
  ylab(bquote('Conc., ppb')) +
  xlab(bquote('Time')) 

### Correlation plot with Linear regression and equation -------------------------

formula <- y ~ x ### Need to keep this so LM regression appears on plot

b <- ggplot(data = data, aes(x = NO2, y = O3)) +
  geom_point(size = 0.5) + 
  scale_x_continuous(expand = c(0, 0), limits = c(0, 45)) + 
  scale_y_continuous(expand = c(0, 0), limits = c(0, 70)) +
  theme_classic() +
  theme(text = element_text(size = 12))+
  xlab(bquote('Conc.' ~NO[2]~', ppb')) +
  ylab(bquote('Conc.' ~O[3]~', ppb')) +
  geom_smooth(method = "lm", formula = formula, se = FALSE) +
    stat_poly_eq(aes(label =  paste(stat(rr.label), sep = "*\", \"*")),
               formula = formula, rr.digits = 4 , parse = TRUE, label.y = 0.25, label.x = 0.95, size = 4)+
  annotate("text", x =35, y = 33, label = "winter")

c <- ggplot(data = dataSummer, aes(x = NO2, y = O3)) +
  geom_point(size = 0.5) + 
  scale_x_continuous(expand = c(0, 0), limits = c(0, 45)) + 
  scale_y_continuous(expand = c(0, 0), limits = c(0, 70)) +
  theme_classic() +
  theme(text = element_text(size = 12))+
  xlab(bquote('Conc.' ~NO[2]~', ppb')) +
  ylab(bquote('Conc.' ~O[3]~', ppb')) +
  geom_smooth(method = "lm", formula = formula, se = FALSE) +
    stat_poly_eq(aes(label =  paste(stat(rr.label), sep = "*\", \"*")),
               formula = formula, rr.digits = 4 , parse = TRUE, label.y = 0.25, label.x = 0.95, size = 4) +
  annotate("text", x =36, y = 33, label = "summer")

# -999 value for example

dataSummer[2,3] = -999
  
d <- ggplot(data = dataSummer, aes(x = NO2, y = O3)) +
  geom_point(size = 0.5) + 
  scale_x_continuous(expand = c(0, 0), limits = c(0, 45)) + 
  scale_y_continuous(expand = c(0, 0), limits = c(-1050, 70)) +
  theme_classic() +
  theme(text = element_text(size = 12))+
  xlab(bquote('Conc.' ~NO[2]~', ppb')) +
  ylab(bquote('Conc.' ~O[3]~', ppb')) +
  geom_segment(aes(x = 22, y = -600, xend = 16.5, yend = -950),
                  arrow = arrow(length = unit(0.5, "cm"))) +
  annotate("text", x = 32, y = -550, label = "error from analysis")

gt <- arrangeGrob(a, b, c, d,                               
             ncol = 1, nrow = 4)
# Add labels to the arranged plots
p <- as_ggplot(gt) +                                # transform to a ggplot
  draw_plot_label(label = c("A", "B", "C", "D"), size = 16,
                  x = c(0, 0, 0, 0), y = c(1, 0.78, 0.53, 0.28)) # Add labels
p

```

# Lab Results 

We were unable to introduce a survey to undergraduate students that would have explicitly addressed their experiences and thoughts with Experiment 1. However, discussing Experiment 1 with the four laboratory TAs revealed two promising insights. Firstly, the TAs found that students' questions were largely related to lab content, and not the technical aspects of Excel. Secondly, student questions that did pertain to Excel were readily addressed by directing them to the supporting *Tip sheet* document. 

Furthermore, after inspection of more than 300 student submitted figures, none had any critical flaws (either in visualization or data analysis). There were instances of several minor issues common to several student plots (i.e. adding a linear regression to their time-series plots), but these have been addressed in the updated Experiment 1 instructions and supporting *Tip Sheet* for the Fall 2020 session.

# Implementation in the Fall Term 

Experiment 1 is slated to be introduced in the upcoming Fall 2020 CHM 135 session, with an estimated >1500 enrolled students. Fortunately, the scope of the NAPS program and the readily scalable coding used to generate student data sets/answer sheets can easily cope with the increase in course participants. As well, a limitation of the summer session was that limitations in the Quercus setup practically limited the entire course to 15 winter and 15 summer data sets. This has been addressed in the upcoming session where each lab section will have it's own unique pool of data sets from a unique NAPS monitoring station in Toronto. While we'd ideally like to assign each student with a unique data set, this method best minimizes overlap in student analysis while remaining logistically practical within the existing Quercus infrastructure. 

# Future Directions & Personal Speculations

We believe the data analysis, numeracy, and graphicacy^[literacy is the ability to read and write; numeracy is the ability to understand and work with numbers; and graphicacy is the ability to understand and present information in the form of charts, graphs and other non-textual formats.] discussed in Experiment 1 will have a role in the CHM 135 laboratory curriculum beyond the Covid19 restrictions on in-person labs. Perhaps this may results in a computational "dry" lab similar to the one conducted in CHM 135, or in shifting the data analysis to student generated data sets (i.e. grouping all data experimental results from one lab group together and have each student analyze it individually). 

Speaking of graphicacy and numeracy, many elements of statistical analysis, interpretation set statistical analysis results, useful Excel functions that did not make it into the lab, and extended discussions on graphicacy were produced for CHM 135. While many of these were removed to streamline the Experiment 1 *tip sheet*, it raises the question of construction an "in-house" reference for undergraduate chemistry students. Such a book can be easily created as a website containing common Excel tutorials in a chemistry specific context. Such a repository would relieve upper year teaching as faculty can simply reference the guidebook rather than individually writing instructions, and students would have a central guide to many of their data analysis questions. This could be fertile grounds for a future CTFP. 

- Dialing in Quercus setup to expand course components to > 1800 students. 
  - can create that many data sets/answer keys easily, bottleneck is upload to quercus. 
- refine Excel operations
- Explicit discussions on data analysis
  - this lab, in my opinion, is more about understanding data analysis than it is chemistry. Once you learn stuff here you can apply in all sorts of ways in upper year courses. 
  - That's why the *Tip Sheet* i wrote was so long. I never expected students to read the entire thing, but if they had any questions they could look it up there. To be fair though, I think the info within that document should prettied up and set up as a departmental wide guide to data visualization/analysis etc. Some of the stuff made by grad students is awful/deceptive. 
- Enhanced discussion on statistics with a focus on interpreting the numbers rather then calculating them with mathematical formulas. 

# Stuff left on the cutting room floor 

A surprisingly difficult aspect of creating Experiment 1 was deciding what *not to* include in the course. While we're largely satisfied with the present course, some of the elements could prove useful for future iterations or upper year classes. These ideas can be readily re-integrated into the existing experimental framework if needed. Some of these ideas are briefly described below. We hope that they inspire readers for future versions of this lab. 

- S0~2~ is the main pollutant responsible for acid rain, and was regulated nationally in 1985 and provincially in 2007. Consequently, we wanted students to investigate if SO~2~ levels have significantly decreased as a result of these regulations. NAPS data of SO~2~ emissions cover more than 50 stations in Ontario and coverage extends as far back as 1975. While a great opportunity to discuss statistical significance, and readily executable in Excel, we struggled with exploring the larger chemistry of this phenomenon in a context appropriate to the CHM 135 course. 
- The current lab utilizes the "add trend line" option in Excel. We had original envisioned using the *Analysis Toolpak* which extends the linear regression beyond a line of best fit and R^2^ to significance of the coefficients (slope) and ANOVA analysis. While the greater exploration of linear regression was promising, the statistical discussion as determined to be beyond first-year chemistry students.  
- Building off of the last point, we had integrated historical ECCC weather data E^[Hourly weather data downloadable here:https://Toronto.weatherstats.ca/download.html] into the analysis. We envisioned students comparing the correlation of O~3~ with NO~2~ and with temperature (or other weather phenomena). This was abandoned for the present winter vs. summer. 

  - There exist an opportunity to expand this component to multiple linear regression utilizing pollutant concentration and various weather readings, although this is more appropriate for an upper year environmental chemistry course. 
  
- Since O~3~ and NO~2~ levels are largely affected by anthropogenic activity, we wanted to analyze the effects of the Covid-9 related "anthropause" on downtown Toronto levels. Unfortunately the NAPS data sets has a one year lag time to publication, and the live feed from the Ontario Ministry of the Environment^[MOE real time air quality data can be found here: http://www.airqualityontario.com/history/summary.php] requires either significant data collection from students or web-scrapping. The former is unreasonable as the entire course could be derailed if the website goes down, and the later is beyond the skills/contract hours of this humble TA. 
  - This may be worth revisiting in upcoming years when the 2020 NAPS data set is released. 
  
- The NAPS program spans the entirety of Canada. We explored comparisons between rural and urban NAPS measurements but found the process quickly escalated beyond a CHM 135 appropriate discussion. 


# Source code and instructions for generating data sets

Generation of all of the data sets and their accompanying answer sheets are executed in R with prodigious use of the *tidyverse* and *Rmarkdown*. As it stands we can easily generate a unique pairing of summer/winter data sets for every student in the Fall 2020 session. The source code and example ECCC data, student data sets, and TA answer reports can all be found on [GitHub](https://github.com/DavidRossHall/CHM135_Exp1Data).^[Git hub repository with source code: [https://github.com/DavidRossHall/CHM135_Exp1Data](https://github.com/DavidRossHall/CHM135_Exp1Data)] 

A more detailed description of the process is described in the Git Hub repo, but briefly:

1. ECCC hourly data for NO~2~ or O~3~ is subset based on a given reporting station; in the present iteration all measurements are from downtown Toronto in 2018.
2. Bilingual headers, ancillary columns, etc. are removed from the ECCC data set, dates are conversed to Excel format, and the remaining data is transformed from the 'wide' matrix style to the 'long' columnar format for easier manipulation in Excel, see table 1.
```{marginfigure}

\begin{center}
\begin{tabular}{ c c c }
 \multicolumn{3}{l}{Table 1: Student data set}\\
 \hline
  Date & NO2 & O3 \\
 \hline
 43110.00 & 18 & 15 \\
 43110.04 & 11 & 21 \\
 43110.08 & 8 & -999 \\
 $\vdots$ & $\vdots$ & $\vdots$ \\
 \hline
\end{tabular}
\end{center}

```
3. A specified number of student data sets are generated from a 7-day moving window of the year-long data. I.e. data set 1 is January 1st to 7th, data set 2 is January 2nd to January 8th. A complimentary summer data set is taken starting from July 1st.
4. A "-999" error is inserted randomly into each student data set. 
5. Data sets saved in a new folder as .csv files. 
6. For each data set generated, a Rmarkdown script generates a PDF with the analysis results. TAs can compare the answer sheet to student submissions. 


Again, all of this code is explained on the Git Hub repo. For the unawares, Git Hub provides hosting for software development, distribution version control, and source code management and is readily [integretated into the RStudio environment](https://support.rstudio.com/hc/en-us/articles/200532077-Version-Control-with-Git-and-SVN). In practice, this means that the code used to automatically generate student data sets and answer keys is preserved online, and can be safely passed along from year to year thanks to version control. The Git Hub environment is ideal for introducing new components and removing old ones from the code thanks to version control, and provides an effective framework to ensure Experiment 1 can be readily changed in its future iterations. 


```{r bib, include=FALSE}
# create a bib file for the R packages used in this document
knitr::write_bib(c('base', 'rmarkdown'), file = 'skeleton.bib')
```
